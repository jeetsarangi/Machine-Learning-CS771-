{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b3c49cb",
   "metadata": {},
   "source": [
    "# Question 1:\n",
    "## Here we are working with the automobile dataset of UCI repository and we will predict price of cars using some numeric and nominal attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3210fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as lin\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b5d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle = pd.read_csv(\"Vehicles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6439e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6061aaef",
   "metadata": {},
   "source": [
    " Here I saw some values are missing and has \"?\" instead so I first replaced them with NaN because then it will be easy to\n",
    " see and work on the data as otherwise i can't see which values are missing clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65249651",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle = vehicle.replace('?',np.nan)\n",
    "vehicle.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d91b7",
   "metadata": {},
   "source": [
    " Then Dropped below the rows that had the price values missing as replacing the price values with mean or median or any other\n",
    " values will not work appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85826b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle.dropna(subset=['price'], axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33218d4",
   "metadata": {},
   "source": [
    " Replacing the mission values of the column \"normalized-losses\" with the mean as it seems to be useful and to fit nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a650303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle['normalized-losses'] = vehicle['normalized-losses'].astype(float, errors = 'raise')\n",
    "mean = vehicle['normalized-losses'].mean()\n",
    "vehicle['normalized-losses'] = vehicle['normalized-losses'].replace(np.nan,mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21694c6",
   "metadata": {},
   "source": [
    "Replacing the \"num-of-doors\" with appropriate numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25baff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle['num-of-doors'] = vehicle['num-of-doors'].replace(\"two\",2)\n",
    "vehicle['num-of-doors'] = vehicle['num-of-doors'].replace(\"four\",4)\n",
    "mode =float( vehicle['num-of-doors'].mode())\n",
    "vehicle['num-of-doors'] = vehicle['num-of-doors'].replace(np.nan,mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01bb650",
   "metadata": {},
   "source": [
    "Now made the \"fuel-type\",\"aspiration\" attributes to 0,1 dummy values as these are boolean attributes so just converted it to numeric boolean representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10f0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle['fuel-type'] = vehicle['fuel-type'].replace(\"diesel\",1)\n",
    "vehicle['fuel-type'] = vehicle['fuel-type'].replace(\"gas\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c950f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle['aspiration'] = vehicle['aspiration'].replace(\"std\",0)\n",
    "vehicle['aspiration'] = vehicle['aspiration'].replace(\"turbo\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52867a32",
   "metadata": {},
   "source": [
    "Replacing the body style attribute with numeric as the price seems sensitive to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle['body-style'] = vehicle['body-style'].replace(\"hardtop\",1)\n",
    "vehicle['body-style'] = vehicle['body-style'].replace(\"wagon\",2)\n",
    "vehicle['body-style'] = vehicle['body-style'].replace(\"sedan\",3)\n",
    "vehicle['body-style'] = vehicle['body-style'].replace(\"hatchback\",4)\n",
    "vehicle['body-style'] = vehicle['body-style'].replace(\"convertible\",5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b4b26",
   "metadata": {},
   "source": [
    "\"Engine-location\" is a boolean attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d99cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle['engine-location'] = vehicle['engine-location'].replace(\"front\",1)\n",
    "vehicle['engine-location'] = vehicle['engine-location'].replace(\"rear\",0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8348905e",
   "metadata": {},
   "source": [
    "\"Engine-type\" is a sensitive feature but is categorical so replaced it with the numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5510e95f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vehicle['engine-type'] = vehicle['engine-type'].replace(\"dohc\",1)\n",
    "vehicle['engine-type'] = vehicle['engine-type'].replace(\"dohcv\",2)\n",
    "vehicle['engine-type'] = vehicle['engine-type'].replace(\"l\",3)\n",
    "vehicle['engine-type'] = vehicle['engine-type'].replace(\"ohc\",4)\n",
    "vehicle['engine-type'] = vehicle['engine-type'].replace(\"ohcf\",5)\n",
    "vehicle['engine-type'] = vehicle['engine-type'].replace(\"ohc\",6)\n",
    "vehicle['engine-type'] = vehicle['engine-type'].replace(\"ohcv\",7)\n",
    "vehicle['engine-type'] = vehicle['engine-type'].replace(\"rotor\",8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6834d7",
   "metadata": {},
   "source": [
    "\"Num-of-cylinders\" is a kind of ordinal attribute so could be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1920c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle['num-of-cylinders'] = vehicle['num-of-cylinders'].replace(\"eight\",8)\n",
    "vehicle['num-of-cylinders'] = vehicle['num-of-cylinders'].replace(\"five\",5)\n",
    "vehicle['num-of-cylinders'] = vehicle['num-of-cylinders'].replace(\"four\",4)\n",
    "vehicle['num-of-cylinders'] = vehicle['num-of-cylinders'].replace(\"six\",6)\n",
    "vehicle['num-of-cylinders'] = vehicle['num-of-cylinders'].replace(\"three\",3)\n",
    "vehicle['num-of-cylinders'] = vehicle['num-of-cylinders'].replace(\"twelve\",12)\n",
    "vehicle['num-of-cylinders'] = vehicle['num-of-cylinders'].replace(\"two\",2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160e9c17",
   "metadata": {},
   "source": [
    "Added mean in place of all the below attributes which had missing values since we can't compute Distance if there are NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459749f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle['bore'] = vehicle['bore'].astype(float, errors = 'raise')\n",
    "mean = vehicle['bore'].mean()\n",
    "vehicle['bore'] = vehicle['bore'].replace(np.nan,mean)\n",
    "vehicle['stroke'] = vehicle['stroke'].astype(float, errors = 'raise')\n",
    "mean = vehicle['stroke'].mean()\n",
    "vehicle['stroke'] = vehicle['stroke'].replace(np.nan,mean)\n",
    "\n",
    "vehicle['horsepower'] = vehicle['horsepower'].astype(float, errors = 'raise')\n",
    "mean = vehicle['horsepower'].mean()\n",
    "vehicle['horsepower'] = vehicle['horsepower'].replace(np.nan,mean)\n",
    "\n",
    "vehicle['peak-rpm'] = vehicle['peak-rpm'].astype(float, errors = 'raise')\n",
    "mean = vehicle['peak-rpm'].mean()\n",
    "vehicle['peak-rpm'] = vehicle['peak-rpm'].replace(np.nan,mean)\n",
    "\n",
    "vehicle['price'] = vehicle['price'].astype(float, errors = 'raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b05497",
   "metadata": {},
   "source": [
    "Dropping many unrelatable and non-numeric columns to make prediction simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d089b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle = vehicle.drop(columns = ['drive-wheels','fuel-system','city-mpg','engine-location','make','engine-type','body-style'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2bc7e3",
   "metadata": {},
   "source": [
    "So,here I normalised all the columns so that all values lie between 0 and 1 to ensure that no feature is in advantage due to high range of gaps between the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c02dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_copy = vehicle.copy()\n",
    "for column in vehicle_copy.drop(columns = ['price']).columns:\n",
    "    vehicle_copy[column] = vehicle_copy[column]/vehicle_copy[column].abs().max()\n",
    "vehicle_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e8e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f39227",
   "metadata": {},
   "source": [
    "Below is a function that can split given dataset into the desired ratio of training:test so that we can use this to split and validate our learner with the validation that is taken out from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_set(data,test_ratio):\n",
    "    permuted = np.random.permutation(len(data))\n",
    "    testsize = int(len(data)*test_ratio)\n",
    "    trainsize = len(data)-testsize\n",
    "    trainindexs = permuted[:trainsize]\n",
    "    testindexs = permuted[trainsize:]\n",
    "    return data.iloc[trainindexs],data.iloc[testindexs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faebc7c2",
   "metadata": {},
   "source": [
    "Below is a function defination which implements the knn algorithm it takes as input set of test-points,k-value,training-set,training-labels and returns the predicted value of the testpoint given as input.This is a regression solution by taking average of all k nearest neighbor's labels or values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ca5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(Test_Point,k,training,train_labels):\n",
    "    \n",
    "    distances = []\n",
    "    n_ara = training.to_numpy()\n",
    "    point = Test_Point.to_numpy()\n",
    "  \n",
    "    \n",
    "    distances = lin.norm(n_ara - point, ord = 2, axis = 1 ) \n",
    "    result = 0\n",
    "    temp = distances.argsort()\n",
    "    nrr = train_labels.to_numpy()\n",
    " \n",
    "    for i in range(k):\n",
    "            result += nrr[temp[i]]\n",
    "    \n",
    "    return result/k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca8555",
   "metadata": {},
   "source": [
    "## 1a)\n",
    "In the next slice of code basically I am splitting the dataset into two parts for training and validation then from training set chopped the price column so that we can use it to calculate distances then added the prices columns to train_label set so that after getting top k neighbours we can use there labels/price values to predict the test point.\n",
    "\n",
    "Then I called knn for all the test points and for each of the values returned I stored them in my predicted array and then using the predicted array and test labels or values calculated the Root Mean Squared Error(RSME) for k value 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c46e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "T1,T2 = split_training_set(vehicle_copy,0.3)\n",
    "training = T1.drop(\"price\",axis=1)\n",
    "train_labels = T1[\"price\"].copy()\n",
    "\n",
    "\n",
    "test = T2.drop(\"price\",axis=1)\n",
    "test_labels = T2[\"price\"].copy()\n",
    "\n",
    "for i in range(len(test)):\n",
    "    predicted.append(knn(test.iloc[i],3,training,train_labels))\n",
    "pred_np = np.array(predicted)\n",
    "RSME = np.sqrt(np.mean((pred_np - test_labels)**2))\n",
    "print(\"a) Root Mean Squared Error(RSME) for k value 3 is \"+str(RSME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7027bd7",
   "metadata": {},
   "source": [
    "In below snippet of code I defined a function called cross_ValidationonK10 which takes input the value of k and for that k splits the dataset 10 times to different training and validation set and calculated the average RSME for that k on average and then return the average RSME value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feeac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_onk10(k):\n",
    "    observations = []\n",
    "    for n in range(20):\n",
    "        T1,T2 = split_training_set(vehicle_copy,0.3)\n",
    "        training = T1.drop(\"price\",axis=1)\n",
    "        train_labels = T1[\"price\"].copy()\n",
    "        test = T2.drop(\"price\",axis=1)\n",
    "        test_labels = T2[\"price\"].copy()\n",
    "        \n",
    "        predicted = []\n",
    "        for i in range(len(test)):\n",
    "            predicted.append(knn(test.iloc[i],k,training,train_labels))\n",
    "        pred_np = np.array(predicted)\n",
    "        RMSE = np.sqrt(np.mean((pred_np - test_labels)**2))\n",
    "        observations.append(RMSE)\n",
    "    return np.mean(observations)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8800a343",
   "metadata": {},
   "source": [
    "Here I made the next function which calls the cross validation for different k values in range of 1-30 and stores all the returned average RSME values an array finally report the best k for which we get least RSME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e2d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k():\n",
    "    error = []\n",
    "    for k in range(1,30):\n",
    "        error.append(cross_validation_onk10(k))\n",
    "    \n",
    "    res = error.index(min(error))+1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb92410",
   "metadata": {},
   "source": [
    "## 1b)\n",
    "Below we are just calling the above function and reporting the best k value for 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54d538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = find_best_k()\n",
    "print(\"b) The best possible value for k which given lowest RSME on average is \"+str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b6b304",
   "metadata": {},
   "source": [
    "The below code implements a graph to show how the algorithm performs on different k values on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22edacf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []\n",
    "for k in range(1,30):\n",
    "    error.append(cross_validation_onk10(k))\n",
    "plt.plot(range(1,30),error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca649e1",
   "metadata": {},
   "source": [
    "# Question 2:\n",
    "## We will be working with the census income dataset from the UCI repository and this is a classification problem for which we'll predict whether a given person has Salary > 50K or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe5641",
   "metadata": {},
   "outputs": [],
   "source": [
    "census = pd.read_csv(\"adult.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303bd4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "census.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ab453",
   "metadata": {},
   "source": [
    "To make the dataset workable I am deleting the rows which has missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f67cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "census = census.replace(' ?',np.nan)\n",
    "census.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88057c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "census =  census.drop(columns = ['marital-status','relationship','race','native-country','workclass'])\n",
    "census.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e614a",
   "metadata": {},
   "source": [
    "Since Education is a very valuable  attribute in this problem I replaced the column with numeric values because in my decision tree algorithm i will take threshold to be a criteria for splitting into two nodes So placing numeric values will do the job quite nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa8f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "census['education'] = census['education'].replace(\" Preschool\",1)\n",
    "census['education'] = census['education'].replace(\" 1st-4th\",5)\n",
    "census['education'] = census['education'].replace(\" 5th-6th\",10)\n",
    "census['education'] = census['education'].replace(\" 7th-8th\",15)\n",
    "census['education'] = census['education'].replace(\" 9th\",20)\n",
    "census['education'] = census['education'].replace(\" 10th\",25)\n",
    "census['education'] = census['education'].replace(\" 11th\",30)\n",
    "census['education'] = census['education'].replace(\" 12th\",35)\n",
    "census['education'] = census['education'].replace(\" HS-grad\",40)\n",
    "census['education'] = census['education'].replace(\" Some-college\",65)\n",
    "census['education'] = census['education'].replace(\" Bachelors\",70)\n",
    "census['education'] = census['education'].replace(\" Masters\",75)\n",
    "census['education'] = census['education'].replace(\" Prof-school\",50)\n",
    "census['education'] = census['education'].replace(\" Assoc-acdm\",55)\n",
    "census['education'] = census['education'].replace(\" Assoc-voc\",60)\n",
    "census['education'] = census['education'].replace(\" Doctorate\",80)\n",
    "census['education'] = census['education'].astype(int, errors = 'raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefc62c4",
   "metadata": {},
   "source": [
    "On the similar lines as I did with \"education\" i applied the same logic for \"occupation\" too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "census.dropna(subset=['occupation'], axis=0,inplace=True)\n",
    "census['occupation'] = census['occupation'].replace(\" Tech-support\",13)\n",
    "census['occupation'] = census['occupation'].replace(\" Craft-repair\",3)\n",
    "census['occupation'] = census['occupation'].replace(\" Other-service\",8)\n",
    "census['occupation'] = census['occupation'].replace(\" Sales\",12)\n",
    "census['occupation'] = census['occupation'].replace(\" Exec-managerial\",4)\n",
    "census['occupation'] = census['occupation'].replace(\" Prof-specialty\",10)\n",
    "census['occupation'] = census['occupation'].replace(\" Handlers-cleaners\",6)\n",
    "census['occupation'] = census['occupation'].replace(\" Machine-op-inspct\",7)\n",
    "census['occupation'] = census['occupation'].replace(\" Adm-clerical\",1)\n",
    "census['occupation'] = census['occupation'].replace(\" Farming-fishing\",5)\n",
    "census['occupation'] = census['occupation'].replace(\" Transport-moving\",14)\n",
    "census['occupation'] = census['occupation'].replace(\" Priv-house-serv\",9)\n",
    "census['occupation'] = census['occupation'].replace(\" Protective-serv\",11)\n",
    "census['occupation'] = census['occupation'].replace(\" Armed-Forces\",2)\n",
    "census['occupation'] = census['occupation'].astype(int, errors = 'raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a37c994",
   "metadata": {},
   "source": [
    "Sex is a important attribute and is boolean in nature so just replaced to represent in a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "census['sex'] = census['sex'].replace(\" Female\",1)\n",
    "census['sex'] = census['sex'].replace(\" Male\",0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f8c0c",
   "metadata": {},
   "source": [
    "In below snippet I am replacing Salary column to -1 and +1 just to make it more compatible to operate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214631ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "census['Salary'] = census['Salary'].replace(\" <=50K\",-1)\n",
    "census['Salary'] = census['Salary'].replace(\" >50K\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3966b7",
   "metadata": {},
   "source": [
    "The Below code snippet is for a class Node which represents a node in any decision tree it has attributes such as depth which is by default assigned to 0,decision stump a tuple of size 2 in which first value represent the feature column which is used for partitioning and second value is for threshold on which we perform partition.\n",
    "\n",
    "This code is inspired from Nisheet Sir's example but with some modifications like here we are using entropy based Information Gain criterion to split,The way I split and find threshold is a bit different I use euclidean distance based approach.\n",
    "\n",
    "Basically here we are testing for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78dcb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__( self, depth = 0, stump = (0,0), parent = None ):\n",
    "        self.depth = depth\n",
    "        self.stump = stump\n",
    "        self.parent = parent\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.isLeaf = True\n",
    "        self.label = 0\n",
    "        \n",
    "    def predict( self, data ):\n",
    "        if self.isLeaf:\n",
    "            return self.label\n",
    "        else:\n",
    "            if data[self.stump[0]] > self.stump[1]:\n",
    "                return self.right.predict( data )\n",
    "            else:\n",
    "                return self.left.predict( data )\n",
    "            \n",
    "    def get_Entropy(self,nPos,nNeg):\n",
    "        tot = nPos+nNeg\n",
    "        if tot == 0:\n",
    "            return 0\n",
    "        pPos = nPos/tot\n",
    "        pNeg = nNeg/tot\n",
    "        t1,t2 = 0,0\n",
    "        if pPos > 0 :\n",
    "            t1 = pPos*np.log2(pPos)\n",
    "        if pNeg > 0 :   \n",
    "            t2 = pNeg*np.log2(pNeg)\n",
    "        ent = -(t1+t2)\n",
    "        return ent\n",
    "    \n",
    "    def getStump( self, X, y ):\n",
    "        n = y.size\n",
    "        Information_gain = float('-inf')\n",
    "        for i in range( X.shape[1] ):\n",
    "            if self.parent is not None and i == self.parent.stump[0]:\n",
    "                continue\n",
    "            \n",
    "            candidateThresholds = np.sort( X[:, i] )\n",
    "            idx = np.argsort( X[:, i] )\n",
    "            \n",
    "            ySorted = y[idx]\n",
    "            yCum = np.cumsum( ySorted )\n",
    "            yCumRev = np.cumsum( ySorted[::-1] )[::-1]\n",
    "            \n",
    "            Thresh = (np.mean(X[y > 0, i]) + np.mean(X[y < 0, i]))/2\n",
    "            index = 1\n",
    "            \n",
    "            for j in range(1,candidateThresholds.size-1):\n",
    "                if(Thresh>candidateThresholds[j]):\n",
    "                    index = j\n",
    "                    break\n",
    "            no_ofpos = int((n+sum(y))/2)\n",
    "            no_ofneg = n-no_ofpos\n",
    "            root_ent = self.get_Entropy(no_ofpos,no_ofneg)\n",
    "            \n",
    "            leftdata = index+1\n",
    "            rightdata = n-leftdata\n",
    "            \n",
    "            pos_left = int((yCum[index]+index+1)/2)\n",
    "            neg_left = index+1-pos_left\n",
    "            ent_left = self.get_Entropy(pos_left,neg_left)\n",
    "            \n",
    "            pos_right = int((yCumRev[index+1]+n-i-1)/2)\n",
    "            neg_right = n-i-1-pos_right\n",
    "            ent_right = self.get_Entropy(pos_right,neg_right)\n",
    "            Info_gain = root_ent - ((leftdata/n)*ent_left + (rightdata/n)*ent_right)\n",
    "            \n",
    "            if Info_gain > Information_gain:\n",
    "                    Information_gain = Info_gain\n",
    "                    bestFeat = i\n",
    "                    bestThresh = Thresh\n",
    "                    \n",
    "        return (bestFeat, bestThresh)\n",
    "    \n",
    "    \n",
    "    def train( self, X, y, maxLeafSize, maxDepth ):\n",
    "        \n",
    "        if y.size < maxLeafSize or self.depth >= maxDepth:\n",
    "            self.isLeaf = True\n",
    "            self.label = np.mean( y )\n",
    "        \n",
    "        else:\n",
    "            self.isLeaf = False\n",
    "            self.stump = self.getStump( X, y )\n",
    "            self.left = Node( depth = self.depth + 1, parent = self )\n",
    "            self.right = Node( depth = self.depth + 1, parent = self )\n",
    "            \n",
    "            temporary = X[:, self.stump[0]] - self.stump[1]\n",
    "            \n",
    "            self.left.train( X[temporary <= 0, :], y[temporary <= 0], maxLeafSize, maxDepth )\n",
    "            self.right.train( X[temporary > 0, :], y[temporary > 0], maxLeafSize, maxDepth )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b983285a",
   "metadata": {},
   "source": [
    "This is a Tree class which consists of a constructor and 3 class attributes and 2 methods one used to train the tree another used for predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d9cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__( self, maxLeafSize = 1, maxDepth = 20 ):\n",
    "        self.root = Node()\n",
    "        self.maxLeafSize = maxLeafSize\n",
    "        self.maxDepth = maxDepth \n",
    "        \n",
    "    def predict( self, data ):\n",
    "        return self.root.predict( np.array( data.to_numpy() ) )\n",
    "    \n",
    "    def train( self, X, y ):\n",
    "        self.root.train( X, y, self.maxLeafSize, self.maxDepth )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b695966",
   "metadata": {},
   "source": [
    "Below Code is about a function defination which on given a dataset and a ratio it divides and returns the dataset after splitting according to that ratio ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_set(data,test_ratio):\n",
    "    permuted = np.random.permutation(len(data))\n",
    "    testsize = int(len(data)*test_ratio)\n",
    "    trainsize = len(data)-testsize\n",
    "    trainindexs = permuted[:trainsize]\n",
    "    testindexs = permuted[trainsize:]\n",
    "    return data.iloc[trainindexs],data.iloc[testindexs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3280e2",
   "metadata": {},
   "source": [
    "## 2 a)\n",
    "In the following snippet of code we are splitting the census dataset into training set and validation set then makes a Decision tree for it, trains the tree on the training dataset.\n",
    "Note: This might give some warning can't able to figure out these so ignore for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b9566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1,T2 = split_training_set(census,0.3)\n",
    "training = T1.drop(\"Salary\",axis=1)\n",
    "train_labels = T1[\"Salary\"].copy()\n",
    "DT = Tree( maxLeafSize = 10, maxDepth = 15 )\n",
    "DT.train(training.to_numpy(),train_labels.to_numpy())\n",
    "test = T2.drop(\"Salary\",axis=1)\n",
    "test_labels = T2[\"Salary\"].copy()\n",
    "predicted = []\n",
    "for i in range(len(test)):\n",
    "    predicted.append(DT.predict(test.iloc[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abad8e",
   "metadata": {},
   "source": [
    "Following code will just check how many predictions hits the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef5d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test_labels.to_numpy()\n",
    "count = 0\n",
    "for i in range(len(predicted)):\n",
    "    if (predicted[i]<0 and x[i]<0) or (predicted[i]>=0 and x[i]>0):\n",
    "        count+=1\n",
    "print(\"The accuracy in terms of %age was \"+str(count/len(x)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce551715",
   "metadata": {},
   "source": [
    "## 2b) Validation part:\n",
    "below function defination takes a dataset input and divides it 5 times and calculates the mean accuracy from given hyperparameter level and leafsize (atmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14530561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_Validate(data,level,leafsize):\n",
    "    accuracy = []\n",
    "    for index in range(1,5):\n",
    "        T1,T2 = split_training_set(data,0.3)\n",
    "        training = T1.drop(\"Salary\",axis=1)\n",
    "        train_labels = T1[\"Salary\"].copy()\n",
    "        DT = Tree( maxLeafSize = leafsize, maxDepth = level )\n",
    "        DT.train(training.to_numpy(),train_labels.to_numpy())\n",
    "        test = T2.drop(\"Salary\",axis=1)\n",
    "        test_labels = T2[\"Salary\"].copy()\n",
    "        predicted = []\n",
    "        for i in range(len(test)):\n",
    "            predicted.append(DT.predict(test.iloc[i]))\n",
    "        count = 0\n",
    "\n",
    "        for i in range(len(predicted)):\n",
    "            if (predicted[i]<0 and x[i]<0) or (predicted[i]>=0 and x[i]>0):\n",
    "                count+=1 \n",
    "        accuracy.append(count/len(predicted)*100)\n",
    "    return np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cca103",
   "metadata": {},
   "source": [
    "Below Function will take a dataset as input and returns the optimal Hyperparameter values those are maximum level and maximum leafsize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_param(data):\n",
    "    optimal_leaf = 5\n",
    "    optimal_level = 5\n",
    "    objective = -9999\n",
    "    for leaf in range(15,20):\n",
    "        for level in range(3,10):\n",
    "            temp = cross_Validate(data,level,leaf)\n",
    "            if temp > objective:\n",
    "                objective  = temp\n",
    "                optimal_leaf = leaf\n",
    "                optimal_level = level\n",
    "    return leaf,level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7317e4",
   "metadata": {},
   "source": [
    "The following function just splits the census data in two parts validation set and training set then by tries learning best hyperparameter values using the above function on the training set the trains the tree using the training set and best Hyperparameter values and the reports by testing it on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ff700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traintobestandValidate():\n",
    "    T1,T2 = split_training_set(census,0.3)\n",
    "    bestleaf,bestlevel = optimal_param(T1)\n",
    "    training = T1.drop(\"Salary\",axis=1)\n",
    "    train_labels = T1[\"Salary\"].copy()\n",
    "    DT = Tree( maxLeafSize = bestleaf, maxDepth = bestlevel )\n",
    "    DT.train(training.to_numpy(),train_labels.to_numpy())\n",
    "    test = T2.drop(\"Salary\",axis=1)\n",
    "    test_labels = T2[\"Salary\"].copy()\n",
    "    predicted = []\n",
    "    for i in range(len(test)):\n",
    "        predicted.append(DT.predict(test.iloc[i]))\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        if (predicted[i]<0 and x[i]<0) or (predicted[i]>=0 and x[i]>0):\n",
    "            count+=1 \n",
    "    print(\"Best possible accuracy is \"+str((count/len(predicted))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157d8276",
   "metadata": {},
   "source": [
    "The Below is just the function calling for to the above function and printing the best possible accuracy mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec765a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "traintobestandValidate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
